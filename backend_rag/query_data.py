import os
import numpy as np
from openai import OpenAI
from flask_cors import CORS
from dotenv import load_dotenv
from flask import Flask, request, jsonify
from langchain_community.vectorstores import Chroma
from load_embedding_function import load_embedding_function

# Create a Flask app.
app = Flask(__name__)

# Enable CORS for all routes.
CORS(app)

# Load env variables
load_dotenv()

# The path to the chroma vector database.
CHROMA_PATH = "vector_db_openai"

# Load the embedding function.
embedding_function = load_embedding_function()

# Prepare the DB.
db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function, collection_metadata={"hnsw:space": "cosine"})


@app.route('/query_data', methods=['POST'])
def query_data():
    # Extract the query text out of the JSON object.
    query_text = request.json['query_text']

    with open("result.txt", "a", encoding="utf-8") as file:
        file.write('-------------------------------------------------------------\n')
        file.write("Query: " + query_text + '\n')
        file.write('-------------------------------------------------------------\n')

    # Get the LLM response together with the references(sources).
    response_text, sources = query_rag(query_text)
    sources_response = f"Sources: {sources}"

    return jsonify({'results': response_text, 'sources': sources_response})


def query_rag(query_text: str):
    # Search the DB.
    results = db.similarity_search_with_score(query_text, k=5)
    context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])

    sources = [doc.metadata.get("id", None) for doc, _score in results]
    with open("result.txt", "a", encoding="utf-8") as file:
        file.write('-------------------------------------------------------------\n')
        file.write('Here follows the context containing the 5 nearest-neighbours:\n')
        file.write('-------------------------------------------------------------\n')
        file.write(context_text + '\n')
        file.write('-------------------------------------------------------------\n')
        file.write('Here follows the cosine similarity scores from the query to each chunk:\n')
        file.write('-------------------------------------------------------------\n')
        for doc, _score in results:
            if isinstance(_score, list):
                for item in _score:
                    file.write(str(1 - item) + '\n')
            else:
                file.write(str(1 - _score) + '\n')
        file.write('-------------------------------------------------------------\n')
        file.write('Here is the sources/references to the documentation:\n')
        file.write('Format: data_landing_zone/{document_name}:page_number:chunk_number\n')
        file.write('-------------------------------------------------------------\n')
        file.write(str(sources) + '\n')
        file.write('-------------------------------------------------------------\n')

    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant designed to provide accurate and informative answers to user questions."},
            {"role": "system", "content": "The user question may be based on a specific context window or be a general inquiry."},
            {"role": "system", "content": "When answering the question, make sure to consider the following context window extracted from a vector database, as it may contain relevant information:"},
            {"role": "system", "content": context_text},
            {"role": "system", "content": "Here is the user's question:"},
            {"role": "user", "content": query_text},
            {"role": "system", "content": "Please provide a detailed and accurate response, utilizing the context provided wherever applicable."}
        ],
        temperature=0.5,
        top_p=0.5
    )

    answer = response.choices[0].message.content

    with open("final_response.txt", "a", encoding="utf-8") as file:
        file.write('-------------------------------------------------------------\n')
        file.write("Query: " + query_text + '\n')
        file.write('-------------------------------------------------------------\n')
        file.write("Here is the answer generated by the LLM based on the context and the question:\n")
        file.write('-------------------------------------------------------------\n')
        file.write(answer + "\n")
        file.write('-------------------------------------------------------------\n')
        file.write('Here is the formatted response, containing the sources/references:\n')
        file.write('-------------------------------------------------------------\n')
        formatted_response = f"Response: {answer}\nSources: {sources}\n"
        file.write(formatted_response)
        file.write('-------------------------------------------------------------\n\n')

    return answer, sources


if __name__ == "__main__":
    app.run(port=5000)
